{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03c11d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "694967f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UberTripsKafkaConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation.maxFileSize\", \"1000000\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0a34551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"D:/Just Data/Uber Real-Time Analytics Pipeline/cleaned_uber_trips\")\n",
    "uncleaned_df = spark.read.parquet(\"D:/Just Data/Uber Real-Time Analytics Pipeline/uber_trips\")\n",
    "# raw_df = spark.read.parquet(\"D:/Just Data/Uber Real-Time Analytics Pipeline/yellow_tripdata_2025-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a4348dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------------+---------------+------------+--------------+---------+----------+---------------------+------------+-------------+---------------+------------+\n",
      "|trip_id|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|congestion_surcharge|Airport_fee|cbd_congestion_fee|kafka_timestamp|kafka_offset|ingestion_time|pickup_ts|dropoff_ts|trip_duration_minutes|total_amount|avg_speed_mph|is_high_quality|processed_at|\n",
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------------+---------------+------------+--------------+---------+----------+---------------------+------------+-------------+---------------+------------+\n",
      "|    488|     488|              0|          488|         0|               488|         488|         488|         488|        488|  488|    488|       488|         488|                  488|                 488|        488|               488|            488|         488|           488|      488|       488|                  488|         488|          488|            488|         488|\n",
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------------+---------------+------------+--------------+---------+----------+---------------------+------------+-------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(col(c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0dbc5fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1027.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 169.0 failed 1 times, most recent failure: Lost task 5.0 in stage 169.0 (TID 574) (MysteryCheetah executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file file:///D:/Just%20Data/Uber%20Real-Time%20Analytics%20Pipeline/uber_trips/part-00000-57261ef2-eede-43c9-9b5a-5f3a814cbd76-c000.snappy.parquet. Column: [passenger_count], Expected: bigint, Found: FLOAT.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [passenger_count], physicalType: FLOAT, logicalType: bigint\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file file:///D:/Just%20Data/Uber%20Real-Time%20Analytics%20Pipeline/uber_trips/part-00000-57261ef2-eede-43c9-9b5a-5f3a814cbd76-c000.snappy.parquet. Column: [passenger_count], Expected: bigint, Found: FLOAT.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [passenger_count], physicalType: FLOAT, logicalType: bigint\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43muncleaned_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muncleaned_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Just Data\\Global_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    886\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    887\u001b[39m \n\u001b[32m    888\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    943\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Just Data\\Global_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    957\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    958\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    959\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    960\u001b[39m     )\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Just Data\\Global_env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Just Data\\Global_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Just Data\\Global_env\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1027.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 169.0 failed 1 times, most recent failure: Lost task 5.0 in stage 169.0 (TID 574) (MysteryCheetah executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file file:///D:/Just%20Data/Uber%20Real-Time%20Analytics%20Pipeline/uber_trips/part-00000-57261ef2-eede-43c9-9b5a-5f3a814cbd76-c000.snappy.parquet. Column: [passenger_count], Expected: bigint, Found: FLOAT.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [passenger_count], physicalType: FLOAT, logicalType: bigint\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file file:///D:/Just%20Data/Uber%20Real-Time%20Analytics%20Pipeline/uber_trips/part-00000-57261ef2-eede-43c9-9b5a-5f3a814cbd76-c000.snappy.parquet. Column: [passenger_count], Expected: bigint, Found: FLOAT.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [passenger_count], physicalType: FLOAT, logicalType: bigint\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "uncleaned_df.select([count(col(c)).alias(c) for c in uncleaned_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a4a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(trip_id=490, VendorID=2, passenger_count=None, trip_distance=2.549999952316284, RatecodeID=None, store_and_fwd_flag='N', PULocationID=231, DOLocationID=107, payment_type=1, fare_amount=17.0, extra=1.0, mta_tax=0.5, tip_amount=4.400000095367432, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 18, 383000), kafka_offset=489, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 58, 12), dropoff_ts=datetime.datetime(2025, 1, 1, 3, 14, 50), trip_duration_minutes=16.633333333333333, total_amount=26.399999618530273, avg_speed_mph=9.198396621581788, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=491, VendorID=2, passenger_count=None, trip_distance=5.809999942779541, RatecodeID=None, store_and_fwd_flag='N', PULocationID=70, DOLocationID=95, payment_type=1, fare_amount=24.700000762939453, extra=6.0, mta_tax=0.5, tip_amount=6.440000057220459, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=0.0, Airport_fee=1.75, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 18, 496000), kafka_offset=490, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 3, 30), dropoff_ts=datetime.datetime(2025, 1, 1, 2, 16, 37), trip_duration_minutes=13.116666666666667, total_amount=40.38999938964844, avg_speed_mph=26.576873944099553, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=492, VendorID=2, passenger_count=None, trip_distance=8.020000457763672, RatecodeID=None, store_and_fwd_flag='N', PULocationID=138, DOLocationID=112, payment_type=1, fare_amount=35.20000076293945, extra=6.0, mta_tax=0.5, tip_amount=8.539999961853027, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=0.0, Airport_fee=1.75, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 18, 601000), kafka_offset=491, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 39, 58), dropoff_ts=datetime.datetime(2025, 1, 1, 3, 1, 26), trip_duration_minutes=21.466666666666665, total_amount=52.9900016784668, avg_speed_mph=22.41615034778666, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=494, VendorID=2, passenger_count=None, trip_distance=3.8399999141693115, RatecodeID=None, store_and_fwd_flag='N', PULocationID=48, DOLocationID=24, payment_type=2, fare_amount=24.0, extra=1.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 18, 857000), kafka_offset=493, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 50, 13), dropoff_ts=datetime.datetime(2025, 1, 1, 3, 11, 20), trip_duration_minutes=21.116666666666667, total_amount=29.0, avg_speed_mph=10.910812700086442, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=495, VendorID=2, passenger_count=None, trip_distance=1.7899999618530273, RatecodeID=None, store_and_fwd_flag='N', PULocationID=263, DOLocationID=162, payment_type=1, fare_amount=10.0, extra=1.0, mta_tax=0.5, tip_amount=3.0, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 19, 8000), kafka_offset=494, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 47, 55), dropoff_ts=datetime.datetime(2025, 1, 1, 2, 55, 1), trip_duration_minutes=7.1, total_amount=18.0, avg_speed_mph=15.1267602410115, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=496, VendorID=2, passenger_count=None, trip_distance=3.7699999809265137, RatecodeID=None, store_and_fwd_flag='N', PULocationID=68, DOLocationID=88, payment_type=1, fare_amount=19.799999237060547, extra=1.0, mta_tax=0.5, tip_amount=4.960000038146973, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 19, 142000), kafka_offset=495, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 27, 21), dropoff_ts=datetime.datetime(2025, 1, 1, 2, 43, 14), trip_duration_minutes=15.883333333333333, total_amount=29.759998321533203, avg_speed_mph=14.241343054916525, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=497, VendorID=2, passenger_count=None, trip_distance=3.5399999618530273, RatecodeID=None, store_and_fwd_flag='N', PULocationID=88, DOLocationID=249, payment_type=1, fare_amount=20.5, extra=1.0, mta_tax=0.5, tip_amount=5.599999904632568, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 19, 254000), kafka_offset=496, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 45, 20), dropoff_ts=datetime.datetime(2025, 1, 1, 3, 3, 25), trip_duration_minutes=18.083333333333332, total_amount=31.100000381469727, avg_speed_mph=11.745621993245068, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=498, VendorID=2, passenger_count=None, trip_distance=2.6600000858306885, RatecodeID=None, store_and_fwd_flag='N', PULocationID=239, DOLocationID=162, payment_type=1, fare_amount=17.0, extra=1.0, mta_tax=0.5, tip_amount=4.400000095367432, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 19, 362000), kafka_offset=497, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 17, 30), dropoff_ts=datetime.datetime(2025, 1, 1, 2, 33, 41), trip_duration_minutes=16.183333333333334, total_amount=26.399999618530273, avg_speed_mph=9.86199825848659, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=499, VendorID=1, passenger_count=None, trip_distance=4.800000190734863, RatecodeID=None, store_and_fwd_flag='N', PULocationID=114, DOLocationID=141, payment_type=1, fare_amount=31.0, extra=3.5, mta_tax=0.5, tip_amount=7.199999809265137, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 19, 468000), kafka_offset=498, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 13, 16), dropoff_ts=datetime.datetime(2025, 1, 1, 2, 46, 26), trip_duration_minutes=33.166666666666664, total_amount=45.70000076293945, avg_speed_mph=8.68341743047513, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124)),\n",
       " Row(trip_id=500, VendorID=1, passenger_count=None, trip_distance=0.699999988079071, RatecodeID=None, store_and_fwd_flag='N', PULocationID=161, DOLocationID=229, payment_type=1, fare_amount=7.900000095367432, extra=3.5, mta_tax=0.5, tip_amount=2.5799999237060547, tolls_amount=0.0, improvement_surcharge=1.0, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0, kafka_timestamp=datetime.datetime(2025, 10, 23, 8, 54, 19, 575000), kafka_offset=499, ingestion_time=datetime.datetime(2025, 10, 23, 8, 54, 20, 22000), pickup_ts=datetime.datetime(2025, 1, 1, 2, 58, 36), dropoff_ts=datetime.datetime(2025, 1, 1, 3, 5, 46), trip_duration_minutes=7.166666666666667, total_amount=17.979999542236328, avg_speed_mph=5.860465016475943, is_high_quality=True, processed_at=datetime.datetime(2025, 10, 23, 8, 58, 5, 624124))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy('trip_id').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf68b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+--------------------+------------+-------------------+-------------------+--------------------+\n",
      "|trip_id|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|     kafka_timestamp|kafka_offset|          pickup_ts|         dropoff_ts|      ingestion_time|\n",
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+--------------------+------------+-------------------+-------------------+--------------------+\n",
      "|      1|       1|           NULL|          1.6|      NULL|                 N|         229|         237|           1|       10.0|  3.5|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|           0|2025-01-01 00:18:38|2025-01-01 00:26:59|2025-10-23 06:52:...|\n",
      "|      2|       1|           NULL|          0.5|      NULL|                 N|         236|         237|           1|        5.1|  3.5|    0.5|      2.02|         0.0|                  1.0|       12.12|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|           1|2025-01-01 00:32:40|2025-01-01 00:35:13|2025-10-23 06:52:...|\n",
      "|      3|       1|           NULL|          0.6|      NULL|                 N|         141|         141|           1|        5.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        12.1|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|           2|2025-01-01 00:44:04|2025-01-01 00:46:01|2025-10-23 06:52:...|\n",
      "|      4|       2|           NULL|         0.52|      NULL|                 N|         244|         244|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|         9.7|                 0.0|        0.0|               0.0|2025-10-23 06:51:...|           3|2025-01-01 00:14:27|2025-01-01 00:20:01|2025-10-23 06:52:...|\n",
      "|      5|       2|           NULL|         0.66|      NULL|                 N|         244|         116|           2|        5.8|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.3|                 0.0|        0.0|               0.0|2025-10-23 06:51:...|           4|2025-01-01 00:21:34|2025-01-01 00:25:06|2025-10-23 06:52:...|\n",
      "|      6|       2|           NULL|         2.63|      NULL|                 N|         239|          68|           2|       19.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        24.1|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|           5|2025-01-01 00:48:24|2025-01-01 01:08:26|2025-10-23 06:52:...|\n",
      "|      7|       1|           NULL|          0.4|      NULL|                 N|         170|         170|           1|        4.4|  3.5|    0.5|      2.35|         0.0|                  1.0|       11.75|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|           6|2025-01-01 00:14:47|2025-01-01 00:16:15|2025-10-23 06:52:...|\n",
      "|      8|       1|           NULL|          1.6|      NULL|                 N|         234|         148|           1|       12.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        19.1|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|           7|2025-01-01 00:39:27|2025-01-01 00:51:51|2025-10-23 06:52:...|\n",
      "|      9|       1|           NULL|          2.8|      NULL|                 N|         148|         170|           1|       19.1|  3.5|    0.5|       3.0|         0.0|                  1.0|        27.1|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|           8|2025-01-01 00:53:43|2025-01-01 01:13:23|2025-10-23 06:52:...|\n",
      "|     10|       2|           NULL|         1.71|      NULL|                 N|         237|         262|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|           9|2025-01-01 00:00:02|2025-01-01 00:09:36|2025-10-23 06:52:...|\n",
      "|     11|       2|           NULL|         2.29|      NULL|                 N|         237|          75|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          10|2025-01-01 00:20:28|2025-01-01 00:28:04|2025-10-23 06:52:...|\n",
      "|     12|       2|           NULL|         0.56|      NULL|                 N|         263|         236|           1|        5.8|  1.0|    0.5|      2.16|         0.0|                  1.0|       12.96|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          11|2025-01-01 00:33:58|2025-01-01 00:37:23|2025-10-23 06:52:...|\n",
      "|     13|       2|           NULL|         1.99|      NULL|                 N|         236|         151|           2|       14.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.2|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          12|2025-01-01 00:42:40|2025-01-01 00:55:38|2025-10-23 06:52:...|\n",
      "|     14|       1|           NULL|          1.1|      NULL|                 N|         229|         141|           2|        7.9|  3.5|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          13|2025-01-01 00:30:07|2025-01-01 00:36:48|2025-10-23 06:52:...|\n",
      "|     15|       1|           NULL|          3.2|      NULL|                 N|         141|         113|           1|       26.1|  3.5|    0.5|       7.8|         0.0|                  1.0|        38.9|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          14|2025-01-01 00:39:55|2025-01-01 01:13:59|2025-10-23 06:52:...|\n",
      "|     16|       1|           NULL|          2.5|      NULL|                 N|         158|         170|           2|       17.7|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          15|2025-01-01 00:16:54|2025-01-01 00:35:12|2025-10-23 06:52:...|\n",
      "|     17|       1|           NULL|          1.9|      NULL|                 N|         164|         229|           1|       16.3|  3.5|    0.5|      4.25|         0.0|                  1.0|       25.55|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          16|2025-01-01 00:43:10|2025-01-01 01:00:03|2025-10-23 06:52:...|\n",
      "|     19|       2|           NULL|         0.71|      NULL|                 N|          79|         107|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.2|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          18|2025-01-01 00:01:41|2025-01-01 00:07:14|2025-10-23 06:52:...|\n",
      "|     20|       1|           NULL|          1.2|      NULL|                 N|         246|          90|           1|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          19|2025-01-01 00:33:12|2025-01-01 00:50:14|2025-10-23 06:52:...|\n",
      "|     21|       2|           NULL|         3.45|      NULL|                 N|         263|         107|           1|       17.7|  1.0|    0.5|      4.54|         0.0|                  1.0|       27.24|                 2.5|        0.0|               0.0|2025-10-23 06:51:...|          20|2025-01-01 00:05:49|2025-01-01 00:20:00|2025-10-23 06:52:...|\n",
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+--------------------+------------+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uncleaned_df.orderBy('trip_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c7f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|mini|\n",
      "+----+\n",
      "| 500|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(max('trip_id').alias('mini')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d6e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|trip_id|counter|\n",
      "+-------+-------+\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('trip_id').agg(count('*').alias('counter')).filter(col('counter') > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110b9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------------+--------------------+------------+--------------------+-------------------+-------------------+---------------------+------------+-----------------+---------------+--------------------+\n",
      "|trip_id|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|congestion_surcharge|Airport_fee|cbd_congestion_fee|     kafka_timestamp|kafka_offset|      ingestion_time|          pickup_ts|         dropoff_ts|trip_duration_minutes|total_amount|    avg_speed_mph|is_high_quality|        processed_at|\n",
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------------+--------------------+------------+--------------------+-------------------+-------------------+---------------------+------------+-----------------+---------------+--------------------+\n",
      "|    430|       2|           NULL|         0.99|      NULL|                 N|         237|         236|           1|        8.6|  1.0|    0.5|       1.5|         0.0|                  1.0|                 2.5|        0.0|               0.0|2025-10-23 06:53:...|         429|2025-10-23 06:53:...|2025-01-01 00:08:02|2025-01-01 00:14:48|    6.766666666666667|        15.1|8.778325207714964|           true|2025-10-23 06:58:...|\n",
      "+-------+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------------+--------------------+------------+--------------------+-------------------+-------------------+---------------------+------------+-----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('trip_id') == 430).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132d3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global_env (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
